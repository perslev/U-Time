"""
Script for training a model on a set of data
Should be called form within a U-Time project directory, call 'ut init' first.
All hyperparameters should be stored in the hparams.yaml file (generated by
ut init).
"""

from argparse import ArgumentParser
import os
from utime import Defaults
from concurrent.futures import ThreadPoolExecutor


def get_argparser():
    """
    Returns an argument parser for this script
    """
    parser = ArgumentParser(description='Test the speed of dataloaders')
    parser.add_argument("--num_batches", type=int, default=100)
    parser.add_argument("--num_threads", type=int, default=1)
    parser.add_argument("--train_queue_type", type=str, default='eager',
                        help="Data queueing type for training data. One of:"
                             " 'eager', 'lazy', 'limitation'. The 'eager' "
                             "queue loads all data into memory up front, the "
                             "'lazy' queue loads data only when needed and the"
                             " 'limitation' queue maintains a buffer of "
                             "loaded data limited according to "
                             "--max_loaded_per_dataset and "
                             "--num_access_before_reload which is recycled "
                             "continuously. Note: with --preprocessed, the "
                             "'eager' queue is always used, as data is not "
                             "loaded from the HDF5 archive even with calls to"
                             " .load() methods.")
    parser.add_argument("--max_loaded_per_dataset", type=int, default=None,
                        help="Set a number of maximum SleepStudyBase objects to"
                             " be kept loaded in memory at any given time per "
                             "dataset. OBS: If training on multiple datasets,"
                             " this means that 'max_loaded_per_dataset' times"
                             " the number of datasets will be the number of"
                             "studies loaded at a given time.")
    parser.add_argument("--num_access_before_reload", type=int, default=50,
                        help="If --max_loaded_per_dataset is set, this value "
                             "determines how many times a SleepStudyBase object "
                             "is accessed (normally for extracting data) "
                             "before the study is unloaded and replaced by "
                             "another random study from the same dataset.")
    parser.add_argument("--preprocessed", action='store_true',
                        help="Run on a pre-processed dataset as output by the "
                             "'ut preprocess' script. Streams data from disk "
                             "which significantly reduces memory load. "
                             "However, running with --preprocessed ignores "
                             "settings such as 'strip_func', "
                             "'quality_control_function', 'scaler' etc. "
                             "(those were applied in the preprocess step). "
                             "Changes to any of those parameters requires a "
                             "rerun of 'ut preprocess' to be effective with "
                             "the 'ut train' --preprocessed flag.")
    return parser


def run(args):
    """
    Run the script according to args - Please refer to the argparser.

    args:
        args:    (Namespace)  command-line arguments
        gpu_mon: (GPUMonitor) Initialized mpunet GPUMonitor object
    """
    from mpunet.logging import ScreenLogger
    from utime.hyperparameters import YAMLHParams
    from utime.utils.scriptutils import assert_project_folder
    from utime.utils.scriptutils.train import (get_train_and_val_datasets,
                                               get_h5_train_and_val_datasets,
                                               get_data_queues,
                                               get_generators)

    project_dir = os.path.abspath("./")
    assert_project_folder(project_dir)

    # Get logger object
    logger = ScreenLogger()
    logger("Args dump: {}".format(vars(args)))

    # Settings depending on --preprocessed flag.
    if args.preprocessed:
        yaml_path = Defaults.get_pre_processed_hparams_path(project_dir)
        dataset_func = get_h5_train_and_val_datasets
        train_queue_type = 'eager'
    else:
        yaml_path = Defaults.get_hparams_path(project_dir)
        dataset_func = get_train_and_val_datasets
        train_queue_type = args.train_queue_type

    # Load hparams
    hparams = YAMLHParams(yaml_path, no_version_control=True)

    # Initialize and load (potentially multiple) datasets
    train_datasets, _ = dataset_func(hparams,
                                     no_val=True,
                                     train_on_val=False,
                                     logger=logger)

    # Get a data loader queue object for each dataset
    train_datasets_queues, val_dataset_queues = get_data_queues(
        train_datasets=train_datasets,
        val_datasets=None,
        train_queue_type=train_queue_type,
        val_queue_type=None,
        max_loaded_per_dataset=args.max_loaded_per_dataset,
        num_access_before_reload=args.num_access_before_reload,
        logger=logger
    )

    # Get sequence generators for all datasets
    train_seq, _ = get_generators(train_datasets_queues,
                                  val_dataset_queues=val_dataset_queues,
                                  hparams=hparams)

    ids = range(args.num_batches)
    pool = ThreadPoolExecutor(args.num_threads)
    batches = pool.map(train_seq.__getitem__, ids)
    import time
    before_time = time.time()
    for b in batches:
        pass
    total_time = time.time() - before_time
    time_per_batch = total_time/args.num_batches
    print("Total time (s): {}".format(total_time))
    print("Per batch  (s): {}".format(time))
    print("Batches per s:  {}".format(1/time_per_batch))


def entry_func(args=None):
    # Get the script to execute, parse only first input
    parser = get_argparser()
    args = parser.parse_args(args)
    try:
        run(args=args)
    finally:
        import tables
        tables.file._open_files.close_all()


if __name__ == "__main__":
    entry_func()
